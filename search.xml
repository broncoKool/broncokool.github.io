<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>GCN学习</title>
      <link href="/2019/12/18/gcn-xue-xi/"/>
      <url>/2019/12/18/gcn-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="一-图卷积神经网络的由来"><a href="#一-图卷积神经网络的由来" class="headerlink" title="一 图卷积神经网络的由来"></a>一 图卷积神经网络的由来</h2><p>传统卷积神经网络对象是具有Euclidean domains data数据，该数据最显著的特征是<em><em>具有规则的空间结构 </em></em>，如图片是规则的正方形，语音是规则的一维序列等，这些特征都可以用一维或二维的矩阵来表示，卷积神经网络处理起来比较高效。  </p><p>CNN的【平移不变性】在【非矩阵结构】数据上不适用</p><blockquote><p><strong>平移不变性（translation invariance）</strong>：比较好理解，在用基础的分类结构比如ResNet、Inception给一只猫分类时，无论猫怎么扭曲、平移，最终识别出来的都是猫，输入怎么变形输出都不变这就是平移不变性，网络的层次越深这个特性会越明显。</p></blockquote><blockquote><p><strong>平移可变性（translation variance）</strong>：针对目标检测的，比如一只猫从图片左侧移到了右侧，检测出的猫的坐标会发生变化就称为平移可变性。当卷积网络变深后最后一层卷积输出的feature map变小，物体在输入上的小偏移，经过N多层pooling后在最后的小feature map上会感知不到，这就是为什么R-FCN原文会说网络变深平移可变性变差。</p></blockquote><p>  <strong>离散卷积本质就是一种加权求和</strong>。CNN中的卷积就是一种离散卷积，本质上就是利用一个共享参数的过滤器（kernel），通过<strong>计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征的提取</strong>，当然<strong>加权系数就是卷积核的权重系数(W)</strong>。  </p><p>  那么卷积核的系数如何确定的呢？是<strong>随机化初值</strong>，<strong>然后根据误差函数通过反向传播梯度下降进行迭代优化</strong>。这是一个关键点，卷积核的参数通过优化求出才能实现特征提取的作用，<strong>GCN的理论很大一部分工作就是为了引入可以优化的卷积参数</strong>。  </p><p>生活中很多数据不具备规则的空间结构，称为Non Euclidean data，如，推荐系统、电子交易、分子结构等抽象出来的图谱。这些图谱中的每个节点连接不尽相同，有的节点有三个连接，有的节点只有一个连接，是不规则的结构。对于这些不规则的数据对象，普通卷积网络的效果不尽人意。CNN卷积操作配合pooling等在结构规则的图像等数据上效果显著，但是如果作者考虑非欧氏空间比如图（即graph，流形也是典型的非欧结构，这里作者只考虑图），就难以选取<strong>固定的卷积核来适应整个图的不规则性，如邻居节点数量的不确定和节点顺序的不确定</strong>。</p><p>例如，社交网络非常适合用图数据来表达，如社交网络中节点以及节点与节点之间的关系，用户A（有ID信息等）、用户B、帖子都是节点，用户与用户之间的关系是关注，用户与帖子之间的关系可能是发布或者转发。通过这样一个图谱，可以分析用户对什么人、什么事感兴趣，进一步实现推荐机制。</p><p>总结一下，图数据中的空间特征具有以下特点：<br>1） <strong>节点特征</strong>：每个节点有自己的特征；（体现在点上）<br>2） <strong>结构特征</strong>：图数据中的每个节点具有结构特征，即节点与节点存在一定的联系。（体现在边上）<br>总地来说，图数据既要考虑<strong>节点信息</strong>，也要考虑<strong>结构信息</strong>，图卷积神经网络就可以自动化地既学习节点特征，又能学习节点与节点之间的关联信息。</p><p>综上所述，GCN是要为除CV、NLP之外的任务提供一种处理、研究的模型。<br>图卷积的核心思想是利用『边的信息』对『节点信息』进行『聚合』从而生成新的『节点表示』。</p><h2 id="二-图卷积两种形式"><a href="#二-图卷积两种形式" class="headerlink" title="二 图卷积两种形式"></a>二 图卷积两种形式</h2><p><strong>GCN的本质目的就是用来提取拓扑图的空间特征</strong>。 而图卷积神经网络主要有两类，一类是基于空间域或顶点域vertex domain（spatial domain）的，另一类则是基于频域或谱域spectral domain的。通俗点解释，空域可以类比到直接在图片的像素点上进行卷积，而频域可以类比到对图片进行傅里叶变换后，再进行卷积。</p><h3 id="2-1-vertex-domain（spatial-domain）：顶点域（空间域）"><a href="#2-1-vertex-domain（spatial-domain）：顶点域（空间域）" class="headerlink" title="2.1 vertex domain（spatial domain）：顶点域（空间域）"></a>2.1 vertex domain（spatial domain）：顶点域（空间域）</h3><p>基于空域卷积的方法直接将卷积操作定义在每个结点的连接关系上，它跟传统的卷积神经网络中的卷积更相似一些。在这个类别中比较有代表性的方法有 Message Passing Neural Networks(MPNN)[1], GraphSage[2], Diffusion Convolution Neural Networks(DCNN)[3], PATCHY-SAN[4]等。</p><h3 id="2-2-spectral-domain：频域方法（谱方法）"><a href="#2-2-spectral-domain：频域方法（谱方法）" class="headerlink" title="2.2 spectral domain：频域方法（谱方法）"></a>2.2 spectral domain：频域方法（谱方法）</h3><p>这就是谱域图卷积网络的理论基础了。这种思路就是希望借助图谱的理论来实现拓扑图上的卷积操作。从整个研究的时间进程来看：首先研究GSP（graph signal processing）的学者定义了graph上的Fourier Transformation，进而定义了graph上的convolution，最后与深度学习结合提出了Graph Convolutional Network。</p><p>基于频域卷积的方法则从图信号处理起家，包括 Spectral CNN[5], Cheybyshev Spectral CNN(ChebNet)[6], 和 First order of ChebNet(1stChebNet)[7]　等</p><p>论文Semi-Supervised Classification with Graph Convolutional Networks就是一阶邻居的ChebNet。</p><p>Q1 什么是Spectral graph theory？<br>Spectral graph theory请参考维基百科的介绍，简单的概括就是借助于图的拉普拉斯矩阵的特征值和特征向量来研究图的性质</p><p>Q2 GCN为什么要利用Spectral graph theory？<br>这是论文（Semi-Supervised Classification with Graph Convolutional Networks）中的重点和难点，要理解这个问题需要大量的数学定义及推导</p><p>过程：</p><blockquote><p>（1）定义graph上的Fourier Transformation傅里叶变换（利用Spectral graph theory，借助图的拉普拉斯矩阵的特征值和特征向量研究图的性质）</p></blockquote><blockquote><p>（2）定义graph上的convolution卷积</p></blockquote><h2 id="三-拉普拉斯矩阵"><a href="#三-拉普拉斯矩阵" class="headerlink" title="三 拉普拉斯矩阵"></a>三 拉普拉斯矩阵</h2><p>拉普拉斯矩阵（Laplacian matrix)）也叫做导纳矩阵、基尔霍夫矩阵或离散拉普拉斯算子，主要应用在图论中，作为一个图的矩阵表示。对于图 $G=(V,E)$ ，其Laplacian 矩阵的定义为 $L=D-A$ ，其中$L$  是Laplacian 矩阵， $D=diag(d)$是顶点的度矩阵（对角矩阵）,$d=rowSum(A)$，对角线上元素依次为各个顶点的度， $A $是图的邻接矩阵。</p><p>Graph Fourier Transformation及Graph Convolution的定义都用到图的拉普拉斯矩阵。</p><p><strong>频域卷积的前提条件是图必须是无向图</strong>，只考虑无向图，那么$L$就是对称矩阵。</p><p><img src="img1.png" alt=""></p><h3 id="3-1-常用的拉普拉斯矩阵"><a href="#3-1-常用的拉普拉斯矩阵" class="headerlink" title="3.1 常用的拉普拉斯矩阵"></a>3.1 常用的拉普拉斯矩阵</h3><h4 id="普通形式的拉普拉斯矩阵"><a href="#普通形式的拉普拉斯矩阵" class="headerlink" title="普通形式的拉普拉斯矩阵"></a>普通形式的拉普拉斯矩阵</h4><p>$$<br>L=D-A<br>$$</p><p>$L$中的元素给定为：<br>$$<br>L_{i,j}=<br>\begin{cases}<br>diag(v_i)\quad i=j\<br>-1 \quad i\neq j \;and \;v_i\;is\;adjacent\;to\;v_j\<br>0\quad otherwise<br>\end{cases}<br>$$<br>  其中$diag(vi)$ 表示顶点 $i$ 的度。 </p><h4 id="对称归一化的拉普拉斯矩阵"><a href="#对称归一化的拉普拉斯矩阵" class="headerlink" title="对称归一化的拉普拉斯矩阵"></a>对称归一化的拉普拉斯矩阵</h4><p>$$<br>L^{sys}=D^{-1/2}LD^{-1/2}=I-D^{-1/2}AD^{-1/2}<br>$$</p><p>矩阵元素定义为：<br>$$<br>L^{sys}_{i,j}=<br>\begin{cases}<br>1\quad i=j\;and\;diag(v_i)\neq 0\<br>-\frac{1}{ \sqrt { diag(v_i)diag(v_j) } } \quad i\neq j \;and \;v_i\;is\;adjacent\;to\;v_j\<br>0\quad otherwise<br>\end{cases}<br>$$<br>很多GCN论文应用的是这种拉普拉斯矩阵</p><h4 id="随机游走归一化拉普拉斯矩阵（Random-walk-normalized-Laplacian）"><a href="#随机游走归一化拉普拉斯矩阵（Random-walk-normalized-Laplacian）" class="headerlink" title="随机游走归一化拉普拉斯矩阵（Random walk normalized Laplacian）"></a>随机游走归一化拉普拉斯矩阵（Random walk normalized Laplacian）</h4><p>$$<br>L^{rw}=D^{-1}L=I-D^{-1}A<br>$$</p><p>矩阵元素定义为：<br>$$<br>L^{sys}_{i,j}=<br>\begin{cases}<br>1\quad i=j\;and\;diag(v_i)\neq 0\<br>-\frac{1}{diag(v_i)} \quad i\neq j \;and \;v_i\;is\;adjacent\;to\;v_j\<br>0\quad otherwise<br>\end{cases}<br>$$</p><h4 id="泛化的拉普拉斯-Generalized-Laplacian"><a href="#泛化的拉普拉斯-Generalized-Laplacian" class="headerlink" title="泛化的拉普拉斯 (Generalized Laplacian)"></a>泛化的拉普拉斯 (Generalized Laplacian)</h4><p>定义为（用的少）：<br>$$<br>\begin{cases}<br>Q_{i,j}&lt;0\quad i=j\;and\;diag(v_i)\neq 0\<br>Q_{i,j}=0 \quad i\neq j \;and \;v_i\;is\;adjacent\;to\;v_j\<br>anynumber\quad otherwise<br>\end{cases}<br>$$<br>一个拉普拉斯矩阵计算例子：</p><p><img src="img2.png" alt=""></p><p>可以看出，标准归一化的拉普拉斯矩阵还是对称的，并且符合前面的公式定义。</p><p>Graph Convolution与Diffusion相似之处，当然从Random walk normalized Laplacian就能看出了两者确有相似之处（其实两者只差一个相似矩阵的变换，可参考Diffusion-Convolutional Neural Networks)<br>其实维基本科对Laplacian matrix的定义上写得很清楚，国内的一些介绍中只有第一种定义。这让我在最初看文献的过程中感到一些的困惑，特意写下来，帮助大家避免再遇到类似的问题。</p><h3 id="3-2-无向图拉普拉斯矩阵性质"><a href="#3-2-无向图拉普拉斯矩阵性质" class="headerlink" title="3.2 无向图拉普拉斯矩阵性质"></a>3.2 无向图拉普拉斯矩阵性质</h3><blockquote><p>（1）拉普拉斯矩阵是半正定矩阵。（最小特征值大于等于0）</p></blockquote><blockquote><p>（2）<strong>特征值中0出现的次数就是图连通区域的个数</strong>。</p></blockquote><blockquote><p>（3）最小特征值是0，因为拉普拉斯矩阵（普通形式：$L=D−A$）每一行的和均为0，并且<strong>最小特征值对应的特征向量是每个值全为1的向量</strong>；</p></blockquote><blockquote><p>（4）最小非零特征值是图的代数连通度。  </p></blockquote><p>证明拉普拉斯矩阵半正定：</p><p><img src="img3.png" alt=""></p><p>所以，对于任意一个属于实向量 $f \in \mathbb{R}^m $ （$f为m∗1$的实数列向量 ），都有此公式成立：</p><p>$$<br>f_TLf= ∑^m_{i,j=1}a_ {ij}(f_i −f_j)^2<br>$$</p><h3 id="3-3-为什么GCN要用拉普拉斯矩阵？"><a href="#3-3-为什么GCN要用拉普拉斯矩阵？" class="headerlink" title="3.3 为什么GCN要用拉普拉斯矩阵？"></a>3.3 为什么GCN要用拉普拉斯矩阵？</h3><blockquote><p>拉普拉斯矩阵是对称矩阵，可以进行特征分解（谱分解）</p></blockquote><blockquote><p>由于卷积在傅里叶域的计算相对简单，为了在graph上做傅里叶变换，需要找到graph的连续的正交基对应于傅里叶变换的基，因此要使用拉普拉斯矩阵的特征向量。</p></blockquote><h3 id="3-4-拉普拉斯矩阵的谱分解（特征分解）"><a href="#3-4-拉普拉斯矩阵的谱分解（特征分解）" class="headerlink" title="3.4. 拉普拉斯矩阵的谱分解（特征分解）"></a>3.4. 拉普拉斯矩阵的谱分解（特征分解）</h3><p>GCN的核心基于拉普拉斯矩阵的谱分解，文献中对于这部分内容没有讲解太多，初学者可能会遇到不少误区，所以先了解一下特征分解。</p><p>特征分解（Eigendecomposition），又称谱分解（Spectral decomposition）是将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法。<strong>只有对可对角化矩阵或有n个线性无关的特征向量的矩阵才可以施以特征分解</strong>。</p><p>不是所有的矩阵都可以特征分解，其充要条件为<strong>n阶方阵存在n个线性无关的特征向量</strong>。</p><p>但是拉普拉斯矩阵是半正定矩阵（半正定矩阵本身就是对称矩阵），有如下三个性质：</p><blockquote><p>对称矩阵一定n个线性无关的特征向量</p></blockquote><blockquote><p>半正定矩阵的特征值一定非负</p></blockquote><blockquote><p>对阵矩阵的不同特征值对应的特征向量相互正交，这些正交的特征向量构成的矩阵为正交矩阵。<br>由上拉普拉斯矩阵对称知一定可以谱分解，且分解后有特殊的形式。</p></blockquote><p>对于拉普拉斯矩阵其谱分解为：</p><p>$$<br>L=U \Lambda U^{-1}=U\left[ \begin{matrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n \\ \end{matrix} \right] U^{-1}<br>$$<br>其中$U=(\vec{u_1},\vec{u_2},\cdots,\vec{u_n}) $是列向量为单位特征向量的矩阵，也就说 $ \vec{u_l} $是列向量，$Λ$是$n$个特征值构成的对角阵。</p><p>由于 $U$ 是正交矩阵，即$UU_T=E$，所以特征分解又可以写成：<br>$$<br>L=U \Lambda U^{-1}=U \Lambda U^{T}<br>$$</p><p>注意，特征分解最右边的是特征矩阵的逆，只是拉普拉斯矩阵是对称矩阵才可以写成特征矩阵的转置。</p><h3 id="3-5-拉普拉斯算子"><a href="#3-5-拉普拉斯算子" class="headerlink" title="3.5 拉普拉斯算子"></a>3.5 拉普拉斯算子</h3><p>定义：拉普拉斯算子是n维欧几里德空间中的一个二阶微分算子，定义为梯度$（∇f）$<br>的散度$（ \nabla \cdot f，即\nabla f \cdot f）$。因此如果$f$是二阶可微的实函数，则f的拉普拉斯算子$∆$定义为：<br>$$<br>∆f=\nabla^2 f=\nabla \cdot \nabla f<br>$$<br>$f$的拉普拉斯算子也是笛卡尔坐标系$x_i$中的所有非混合二阶偏导数：</p><p>$$<br>∆f=\sum_{i=1}^n \frac{\partial^2f}{\partial x_i^2}<br>$$</p><p>函数$f$的拉普拉斯算子也是该函数的海塞矩阵（是一个多元函数的二阶偏导数构成的方阵）的迹：<br>$$<br>∆f=tr(H(f))<br>$$<br>拉普拉斯算子（Laplacian operator）的物理意义是空间二阶导，准确定义是：标量梯度场中的散度，一般可用于描述物理量的流入流出。比如说在二维空间中的温度传播规律，一般可以用拉普拉斯算子来描述。</p><p>拉普拉斯矩阵也叫做离散的拉普拉斯算子。</p><h2 id="四-卷积"><a href="#四-卷积" class="headerlink" title="四 卷积"></a>四 卷积</h2><h3 id="4-1-连续形式的一维卷积"><a href="#4-1-连续形式的一维卷积" class="headerlink" title="4.1 连续形式的一维卷积"></a>4.1 连续形式的一维卷积</h3><p>  在泛函分析中，卷积是通过两个函数f(x)和g(x)生成第三个函数的一种算子，它代表的意义是：<strong>两个函数中的一个(取g(x)，可以任意取)函数，把g(x)经过翻转平移,然后与f(x)的相乘，得到的一个新的函数，对这个函数积分，也就是对这个新的函数求它所围成的曲边梯形的面积</strong>  </p><p>设$f(t),g(t)$是两个可积函数，$f(t)与g(t)$的卷积记为$f(t)*g(t)$<br>$$<br>f(t)∗g(t)=∫^{−∞}_{+∞}f(τ)g(t−τ)dτ=∫^{−∞}_{+∞}f(t−τ)g(τ)dτ<br>$$<br><img src="img4.png" alt=""></p><h3 id="4-2-离散形式的一维卷积"><a href="#4-2-离散形式的一维卷积" class="headerlink" title="4.2 离散形式的一维卷积"></a>4.2 离散形式的一维卷积</h3><p>对于定义在整数$Z$上的函数$f,g$,卷积定义为</p><p>$$<br>(f∗g)(t)=∑_{τ=−∞}^∞f(τ)g(t−τ)<br>$$</p><h3 id="4-3-实例-掷骰子问题"><a href="#4-3-实例-掷骰子问题" class="headerlink" title="4.3 实例:掷骰子问题"></a>4.3 实例:掷骰子问题</h3><p>光看数学定义可能会觉得非常抽象，下面举一个掷骰子的问题，该实例参考了知乎问题”如何通俗易懂地解释卷积”的回答。</p><p>想象现在有两个骰子，两个骰子分别是$f$跟$g$，$f(1)$表示骰子$f$向上一面为数字1的概率。同时抛掷这两个骰子1次，它们正面朝上数字和为4的概率是多少呢？相信读者很快就能想出它包含了三种情况，分别是：</p><ul><li><p>f向上为1，g向上为3；</p></li><li><p>f向上为2，g向上为2；</p></li><li><p>f向上为3，g向上为1；</p><p>最后这三种情况出现的概率和即问题的答案，如果写成公式，就是$∑_{τ=1}^3f(τ)g(4−τ)$。可以形象地绘制成下图：  </p></li></ul><p><img src="img5.png" alt=""></p><p>如果稍微扩展一点，比如说认为 $f(0) $或者$ g(0)$等是可以取到的，只是它们的值为$0$而已。那么该公式可以写成$  ∑_{τ=−∞}^∞f(τ)g(4−τ)$。</p><p>[<em>f</em>(−∞),⋯,<em>f</em>(1),⋯,<em>f</em>(∞)]⋅[<em>g</em>(∞),⋯,<em>g</em>(3),⋯,<em>g</em>(−∞)]</p><p>仔细观察，这其实就是卷积$(f∗g)(4)$。如果将它写成内积的形式，卷积其实就是</p><p>$$<br>[f(−∞),⋯,f(1),⋯,f(∞)]⋅[g(∞),⋯,g(3),⋯,g(−∞)]<br>$$<br>这么一看，是不是就对卷积的名字理解更深刻了呢?<strong>所谓卷积，其实就是把一个函数卷(翻)过来，然后与另一个函数求内积</strong>。</p><p>对应到不同方面，卷积可以有不同的解释：$g$ 既可以看作我们在深度学习里常说的核(Kernel)，也可以对应到信号处理中的滤波器(Filter)。而$ f $可以是我们所说的机器学习中的特征(Feature)，也可以是信号处理中的信号(Signal)。$f$和$g$的卷积$ (f∗g)$就可以看作是对$f$的加权求和。下面两个动图就分别对应信号处理与深度学习中卷积操作的过程：</p><p><img src="gif2.gif" alt=""></p><p><img src="gif1.gif" alt=""></p><h2 id="五-傅里叶变换"><a href="#五-傅里叶变换" class="headerlink" title="五 傅里叶变换"></a>五 傅里叶变换</h2><h3 id="5-1-连续形式的傅立叶变换"><a href="#5-1-连续形式的傅立叶变换" class="headerlink" title="5.1 连续形式的傅立叶变换"></a>5.1 连续形式的傅立叶变换</h3><p>关于傅立叶变换相关的详细而有趣的内容，可以看这几篇文章：</p><ul><li><p><a href="https://blog.csdn.net/yyl424525/article/details/98764320" target="_blank" rel="noopener">如何直观地理解傅立叶变换？</a></p></li><li><p><a href="https://blog.csdn.net/yyl424525/article/details/98765291" target="_blank" rel="noopener">如何理解傅立叶级数公式？</a> （强烈推荐看一看）</p></li><li><p><a href="https://blog.csdn.net/yyl424525/article/details/98765466" target="_blank" rel="noopener">从傅立叶级数到傅立叶变换</a>  （强烈推荐看一看）</p></li><li><p><a href="https://blog.csdn.net/yyl424525/article/details/98790080" target="_blank" rel="noopener">如何理解拉普拉斯变换?</a></p><p>下面是一个大致流程：<br>任意函数可以分解为奇偶函数之和:<br>$$<br>f(x)=2f(x)+f(−x)+2f(x)−f(−x)=feven+fodd<br>$$<br>任意一个周期函数可以由若干个正交函数（由$sin,cos$ 构成）的线性组合构成，写出傅里叶级数的形式如下：  </p></li></ul><p>$$<br>f(x)=a_0+∑^∞_{n=1}(a_ncos(\frac{2πn}{T}x)+b_nsin(\frac{2πn}{T}x)),a0∈R<br>$$<br>  利用欧拉公式  $e^{ix}=cos\,x+i\,sin\,x$  （这里的指复数中的$i$），$cosx,sinx$可表示成  :<br>$$<br>cosx=\frac{e^{ix}+e^{−ix}}{2},sinx=\frac{e^{ix}-e^{−ix}}{2i}<br>$$<br>  在时间$t$轴上，把$e^{it}$向量的虚部（也就是纵坐标）记录下来，得到的就是$sin(t)$：  </p><p><img src="gif5.gif" alt=""></p><p>  在时间$t$轴上，把$e^{i2t}$向量的虚部记录下来，得到的就是$sin(2t)$：  </p><p><img src="gif6.gif" alt=""></p><p>  如果在时间$t$轴上，把$e^{it}$的实部（横坐标）记录下来，得到的就是$cos(t)$的曲线：</p><p>  <img src="gif7.gif" alt=""></p><p>  更一般的,具有两种看待$sin,cos$的角度：<br>$$<br>e^{iωt}⟺<br>\begin{cases}<br>sin(wt)\<br>cos(wt)<br>\end{cases}<br>$$<br>这两种角度，一个可以观察到旋转的频率，所以称为频域；一个可以看到流逝的时间，所以称为时域：</p><p><img src="img6.png" alt=""></p><p>所以，任意周期函数可以以$e^{iωt}$为基函数用傅里叶级数的指数形式表示。即，对于一个周期函数$f(x)$以傅里叶级数的指数形式表示为：  </p><p>$$<br>f(x)=∑^∞_{n=−∞}\underbrace{cn}_{基的坐标}⋅\underbrace{ e^{ i\frac{2πnx}{T} } }_{正交基}<br>$$<br>但是对于非周期函数，并不能用傅里叶级数的形式表示。但是还是用某个周期函数$f_T(x)$当$T→∞$来逼近，即$lim_T→∞f_T(x)=f(x)$，用积分的形式可以表示为：<br>$$<br>f(x)=∫_{−∞}^∞​[∫_{−∞}^{+∞}​f(x)e^{−iωx}dx] e^{iωx}dω=∫_{−∞}^∞​F(ω) e^{iωx}dω<br>$$<br>  其中，$F(ω)$就是$f(x)$的连续形式的傅里叶变换：<br>$$<br>F(ω)=F[f(x)]=∫_{−∞}^{+∞}​f(x)e^{−iωx}dx<br>$$<br>  可以看出，$f(x)$和$F(ω)$可以通过指定的积分运算相互表达。  </p><p>  当然，也可以利用欧拉公式通过cos和sin函数表示为<em>F</em>(<em>u</em>)：<br>$$<br>F(u)=∫_{−∞}^{+∞}f(x)[cos(2πxu)−isin(2πxu)]dx<br>$$<br>  所以，对函数<em>f</em>(<em>x</em>)的傅里叶变换F和傅里叶的逆变换F−1记作：<br>$$<br>F(ω)=F[f(x)],f(x)=F−1[F(ω)]<br>$$</p><ul><li><p>$F(ω)$叫做$f(x)$<em>*象函数或傅里叶变换</em> ，即通过傅里叶变换后关于频率的函数，函数图像就是频谱图，<em>ω</em> 就是$f$对应在频域中的频率  </p></li><li><p>$f(x)$叫做$F(ω)  $的原象函数  </p></li></ul><p>其实可以发现这个$f(x)$的傅立叶变换$F(ω)$形式上是$f(x)$与基函数$e^{−iωx}$的积分，本质上将函数$f(x)$映射到了以$e^{−iωx}$为基向量的空间中。  </p><h3 id="5-2-频域（frequency-domain）和时域（time-domain）的理解"><a href="#5-2-频域（frequency-domain）和时域（time-domain）的理解" class="headerlink" title="5.2 频域（frequency domain）和时域（time domain）的理解"></a>5.2 频域（frequency domain）和时域（time domain）的理解</h3><blockquote><p>时域：真实量到的信号的时间轴，代表真实世界。</p></blockquote><blockquote><p>频域：为了做信号分析用的一种数学手段。</p></blockquote><p>要理解时域和频域只需要看下面两张动图就可以了：</p><p><img src="gif8.gif" alt=""></p><p><img src="gif9.gif" alt=""></p><p>频谱图里的竖线分别代表了不同频率的正弦波函数，也就是之前的基，而高度则代表在这个频率上的振幅，也就是这个基上的坐标分量。</p><p>很多在时域看似不可能做到的数学操作，在频域相反很容易。这就是需要傅里叶变换的地方。尤其是从某条曲线中去除一些特定的频率成分，这在工程上称为滤波，是信号处理最重要的概念之一，只有在频域才能轻松的做到。</p><p>看一个傅里叶变换去噪的例子：<br>在傅里叶变换前，图像上有一些规律的条纹，直接在原图上去掉条纹有点困难，但我们可以将图片通过傅里叶变换变到频谱图中，频谱图中那些规律的点就是原图中的背景条纹。</p><p>只要在频谱图中擦除这些点，就可以将背景条纹去掉。  </p><h3 id="5-3-周期性离散傅里叶变换（Discrete-Fourier-Transform-DFT）"><a href="#5-3-周期性离散傅里叶变换（Discrete-Fourier-Transform-DFT）" class="headerlink" title="5.3 周期性离散傅里叶变换（Discrete Fourier Transform, DFT）"></a>5.3 周期性离散傅里叶变换（Discrete Fourier Transform, DFT）</h3><p>傅里叶变换有连续时间非周期傅里叶变换，连续时间周期性傅里叶变换，离散时间非周期傅里叶变换和离散时间周期性傅里叶变换，鉴于计算机主要处理离散周期性信号，本文主要介绍周期性离散时间傅里叶变换（DFT）。信号<br>$x_n$的傅里叶变换$X_k$为:<br>$$<br>X_k​=∑^{N−1}_{n=0}​x_n​e^{ −i\frac{2π}{N} kn }<br>$$<br>  信号$x_n$用其傅里叶变换$X_k$表示为：<br>$$<br>x_n=∑^{N−1}_{n=0}​X_k​e^{i\frac{2π}{N}kn}<br>$$<br>其中</p><ul><li>N表示傅里叶变换的点数</li><li>k表示傅里叶变换的第k个频谱</li></ul><h2 id="六-Graph上的傅里叶变换及卷积"><a href="#六-Graph上的傅里叶变换及卷积" class="headerlink" title="六 Graph上的傅里叶变换及卷积"></a>六 Graph上的傅里叶变换及卷积</h2><p><strong>把传统的傅里叶变换以及卷积迁移到Graph上来，核心工作其实就是把拉普拉斯算子的特征函数</strong></p><p><strong>$e^{−iωt}$ 变为Graph对应的拉普拉斯矩阵的特征向量。</strong></p><p>傅立叶变换与拉普拉斯矩阵的关系：<strong>传统傅立叶变换的基，就是拉普拉斯矩阵的一组特征向量。</strong></p><h3 id="6-1-图上的傅里叶变换"><a href="#6-1-图上的傅里叶变换" class="headerlink" title="6.1 图上的傅里叶变换"></a>6.1 图上的傅里叶变换</h3><p>前面讲到可以用一组正交函数$cos$和$sin$（或$e^{−iωt}$ ）表示任意函数，且傅里叶变换是连续形式的，在处理Graph时，用到的是傅里叶变换的离散形式。由于拉普拉斯矩阵进行谱分解以后，可以得到n个线性无关的特征向量，构成空间中的一组正交基，因此归一化拉普拉斯矩阵算子的特征向量构成了图傅里叶变换的基。图傅里叶变换将输入图的信号投影到了正交空间，相当于把图上定义的任意向量，表示成了拉普拉斯矩阵特征向量的线性组合。</p><p><img src="img7.png" alt=""></p><p>离散积分就是一种内积形式，由此可以定义Graph上的傅里叶变换（实数域）：  </p><p>$$<br>F(λ_l)=\hat{f}(λ_l)=∑^N_{i=1}f(i)u_l(i)<br>$$</p><ul><li><p>$f$是Graph上的N维向量，可以表示某个点的特征向量，f(i)  表示第$i $个特征  </p></li><li><p>$u_l(i) $ 表示第$i $个特征  zhezhe</p></li><li><p>$f$的Graph 傅里叶变换就是与 $λ_l $ 对应的特征向量$u_l$进行内积运算  </p></li><li><p>$λ$就对应于$ω$(具体解释见第7部分)</p></li><li><p>$\hat{f}$  表示$f$的傅里叶变换  </p></li></ul><h3 id="6-2-图的傅立叶变换——图的傅立叶变换的矩阵形式"><a href="#6-2-图的傅立叶变换——图的傅立叶变换的矩阵形式" class="headerlink" title="6.2 图的傅立叶变换——图的傅立叶变换的矩阵形式"></a>6.2 图的傅立叶变换——图的傅立叶变换的矩阵形式</h3><p>利用矩阵乘法将Graph上的傅里叶变换推广到矩阵形式：</p><p><img src="img8.png" alt=""></p><p>即 f 在Graph上傅里叶逆变换的矩阵形式为： </p><p>$$<br>\hat{f}=U_Tf<br>$$</p><h3 id="6-3-图的傅立叶逆变换——图的傅立叶逆变换的矩阵形式"><a href="#6-3-图的傅立叶逆变换——图的傅立叶逆变换的矩阵形式" class="headerlink" title="6.3 图的傅立叶逆变换——图的傅立叶逆变换的矩阵形式"></a>6.3 图的傅立叶逆变换——图的傅立叶逆变换的矩阵形式</h3><p>类似地，传统的傅里叶变换是对频率ω 求积分：</p><p>$$<br>F^{−1}[F(ω)]=\frac{1}{2Π}​∫​F(ω)e^{iωt}dω<br>$$<br>迁移到Graph上变为对特征值$λ_l$求和：<br>$$<br>f(i)=∑^N_{l=1}​\hat{f}​(λ_l​)u_l​(i)<br>$$<br>利用矩阵乘法将Graph上的傅里叶变换推广到矩阵形式：  </p><p><img src="img9.png" alt=""></p><p>  即$ f $在Graph上傅里叶变换的矩阵形式为：<br>$$<br>f=U\hat{f}​<br>$$</p><h3 id="6-4-图上的傅里叶变换推广到图卷积"><a href="#6-4-图上的傅里叶变换推广到图卷积" class="headerlink" title="6.4 图上的傅里叶变换推广到图卷积"></a>6.4 图上的傅里叶变换推广到图卷积</h3><p>在上面的基础上，利用卷积定理类比来将卷积运算，推广到Graph上。</p><p>卷积定理：函数卷积的傅里叶变换是函数傅立叶变换的乘积，即对于函数f与g两者的卷积是其函数傅立叶变换乘积的逆变换（中间的桥梁就是傅立叶变换与反傅立叶变换，证明见：<a href="https://zhuanlan.zhihu.com/p/54505069）：" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54505069）：</a><br>$$<br>f∗g=F^{−1}{F(f)⋅F(g)}=F^{−1}{f^​⋅g^​}<br>$$<br>所以，对图上$f$和卷积核$g$的卷积可以表示为：<br>$$<br>(f∗g)_G=U((U^Tg)\cdot(U^Tf))<br>$$<br> 从上面可以看出，对卷积核g和f进行傅里叶变换的结果$U^T_g,U^T_f$都是一个列向量：  </p><p><img src="img10.png" alt=""></p><p> 所以，很多论文中的Graph卷积公式也写作：<br>$$<br>(f∗g)_G​=U((U^Tg)\odot(U^Tf))<br>$$<br>⊙ 表示hadamard product（哈达马积），对于两个向量，就是进行内积运算；对于维度相同的两个矩阵，就是对应元素的乘积运算。如果把$U^T_g$整体看作可学习的卷积核，这里我们把它写作$g_θ$，  最终图上的卷积公式即是：  </p><p>$$<br>(f∗g)G=Ug_θU^Tf<br>$$<br>有的地方，还把$g_θ=U^Tg$也成对角矩阵的形式，即定义一个滤波$g_θ=diag(U^Tg)$，则：  </p><p><img src="img11.png" alt=""></p><p>接下来第8节要介绍的图上频域卷积的工作，都是在$g_θ$的基础上做文章。  </p><h3 id="七-为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？特征值表示频率？"><a href="#七-为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？特征值表示频率？" class="headerlink" title="七 为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？特征值表示频率？"></a>七 为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？特征值表示频率？</h3><p>在Chebyshev论文（M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural networks on graphs with fast localized spectral filtering,”in Advances in Neural Information Processing Systems, 2016）中就有说明，图上进行傅里叶变换时，拉普拉斯矩阵是对称矩阵，所有有n个线性无关的特征向量，因此可以构成傅里叶变换的一组基，而其对应的特征值就是傅里叶变换的频率。</p><h3 id="7-1-为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？"><a href="#7-1-为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？" class="headerlink" title="7.1 为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？"></a>7.1 为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？</h3><p>傅里叶变换一个本质理解就是：把任意一个函数表示成了若干个正交函数（由sin,cos 构成）的线性组合。</p><p><img src="img12.png" alt=""></p><p>通过即$ f $在Graph上傅里叶逆变换的矩阵形式：$f=Uf^(b)$也能看出，graph傅里叶变换把graph上定义的任意向量$f$，表示成了拉普拉斯矩阵特征向量的线性组合，即：  </p><p>$$<br>f=\hat{f}(1)u_1+\hat{f}(2)u_2+⋯+\hat{f}(n)u_n<br>$$<br> 那么：为什么graph上任意的向量f都可以表示成这样的线性组合？</p><p>原因在于$(\vec{u_1},\vec{u_2},⋯,\vec{u_n})  $  是graph上 n维空间中的 n 个线性无关的正交向量（<strong>拉普拉斯矩阵是对称矩阵，必定可以进行特征分解，有n个线性无关的特征向量</strong>），由线性代数的知识可以知道：n 维空间中n 个线性无关的向量可以构成空间的一组基，而且拉普拉斯矩阵的特征向量还是一组正交基。  </p><h3 id="7-2-怎么理解拉普拉斯矩阵的特征值表示频率？"><a href="#7-2-怎么理解拉普拉斯矩阵的特征值表示频率？" class="headerlink" title="7.2 怎么理解拉普拉斯矩阵的特征值表示频率？"></a>7.2 怎么理解拉普拉斯矩阵的特征值表示频率？</h3><p>在graph空间上无法可视化展示“频率”这个概念，那么从特征方程上来抽象理解。</p><p>将拉普拉斯矩阵 L 的 n 个非负实特征值，从小到大排列为$λ1≤λ2≤⋯≤λn$，  而且最小的特征值$λ1=0$，因为 n 维的全1向量对应的特征值为0（由 L 的定义就可以得出）：  </p><p><img src="img13.png" alt=""></p><p>从特征方程的数学理解来看：<br>$$<br>Lu=λu<br>$$<br>在由Graph确定的 n 维空间中，越小的特征值 $λ_l$表明：拉普拉斯矩阵 L 其所对应的基$u_l$上的分量、“信息”越少，那么当然就是可以忽略的低频部分了。</p><p>其实图像压缩就是这个原理，把像素矩阵特征分解后，把小的特征值（低频部分）全部变成0，PCA降维也是同样的，把协方差矩阵特征分解后，按从大到小取出前K个特征值对应的特征向量作为新的“坐标轴”。</p><p>Graph Convolution的理论告一段落了，下面开始Graph Convolution Network</p><h2 id="八-深度学习中GCN的演变"><a href="#八-深度学习中GCN的演变" class="headerlink" title="八 深度学习中GCN的演变"></a>八 深度学习中GCN的演变</h2><p>Deep learning 中的Graph Convolution直接看上去会和第6节推导出的图卷积公式有很大的不同，但是万变不离其宗，都是根据下式来推导的：</p><p><img src="img14.png" alt=""></p><p>Deep learning 中的Convolution就是要<strong>设计含有trainable共享参数的kernel</strong>。</p><p>上式<strong>计算量很大</strong>，因为<strong>特征向量矩阵U 的复杂度</strong>是$O(N^2)$。此外，对于大型图来说，<strong>L特征值分解的计算量也很大</strong>。</p><p>基于上面最原始的卷积公式，深度学习中的GCN主要是从下面几篇文章演变而来的（引用次数都很高），后面一一进行简单介绍：</p><ul><li>【1】Bruna, Joan, et al. “Spectral networks and locally connected networks on graphs.” 源于ICLR 2014（被引740次）</li><li>【2】Defferrard, Michaël, Xavier Bresson, and Pierre Vandergheynst. “Convolutional neural networks on graphs with fast localized spectral filtering.” 源于NIPS 2016（被引997次）</li><li>【3】Hammond, David K., Pierre Vandergheynst, and Rémi Gribonval. “Wavelets on graphs via spectral graph theory.” Applied and Computational Harmonic Analysis 30.2 (2011)（被引874次）</li><li>【4】Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” 源于ICML 2017 （被引1537次）</li></ul><h3 id="8-1-Spectral-CNN"><a href="#8-1-Spectral-CNN" class="headerlink" title="8.1 Spectral CNN"></a>8.1 Spectral CNN</h3><p>谱CNN源于论文（J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally connected networks on graphs,” in Proceedings of International Conference on Learning Representations, 2014），Bruna等人，第一次提出谱卷积神经网络。他们简单地把$g_θ $看作是一个可学习参数的集合：  $gθ=Θ_{i,j}^k$。并且假设图信号是多维的，图卷积层顶定义为：  </p><p><img src="img15.png" alt=""></p><p><img src="img16.png" alt=""></p><p>第一代的参数方法存在着一些弊端，主要在于：<br>  （1）<strong>计算复杂</strong>：如果一个样本一个图，那么每个样本都需要进行图的拉普拉斯矩阵的特征分解求U矩阵计算复杂；每一次前向传播，都要计算$U,diag(θ_l) $   及S$U^T$三者的乘积，特别是对于大规模的graph，计算的代价较高，需要$O(n^2)$的计算复杂度  </p><p>  （2）<strong>是非局部性连接的</strong><br>  （3）卷积核需要N个参数，当图中的节点N很大时是不可取的</p><p>由于以上的缺点第二代的卷积核设计应运而生。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GCN </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlp文本聚类</title>
      <link href="/2019/12/17/nlp-wen-ben-ju-lei/"/>
      <url>/2019/12/17/nlp-wen-ben-ju-lei/</url>
      
        <content type="html"><![CDATA[<h1 id="NLP文本聚类"><a href="#NLP文本聚类" class="headerlink" title="NLP文本聚类"></a>NLP文本聚类</h1><p>组内工程项目要做生成式文本摘要，前期做了一些文本预处理和评测工具调研工作，这周分配了文本聚类的任务，还是从调研开始。</p><h2 id="一、文本聚类定义"><a href="#一、文本聚类定义" class="headerlink" title="一、文本聚类定义"></a>一、文本聚类定义</h2><p>​    文本聚类主要是依据著名的聚类假设：同类的文档相似度较大，而不同类的文档相似度较小。作为一种无监督的机器学习方法，聚类由于不需要训练过程，以及不需要预先对文档手工标注类别，因此具有一定的灵活性和较高的自动化处理能力，已经成为对文本信息进行有效地组织、摘要和导航的重要手段。</p><h2 id="二、算法分类"><a href="#二、算法分类" class="headerlink" title="二、算法分类"></a>二、算法分类</h2><h3 id="1-划分法-基于划分的聚类方法"><a href="#1-划分法-基于划分的聚类方法" class="headerlink" title="1 划分法(基于划分的聚类方法)"></a>1 划分法(基于划分的聚类方法)</h3><p>​        给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N。而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据纪录；（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的记录越远越好。  </p><p>​        使用这个基本思想的算法有：K-Means算法、K-MEDOIDS算法、CLARANS算法。</p><p>​        K-means算法是一种典型的基于划分的聚类算法，该聚类算法的基本思想是在聚类开始时根据用户预设的类簇数目k随机地在所有文本集当中选择k个对象，将这些对象作为k个初始类簇的平均值或者中心，对于文本集中剩余的每个对象，根据对象到每一个类簇中心的欧几里得距离，划分到最近的类簇中；全部分配完之后，重新计算每个类簇的平均值或者中心，再计算每篇文本距离这些新的类簇平均值或中心的距离，将文本重新归入目前最近的类簇中；不断重复这个过程，直到所有的样本都不能再重新分配为止。</p><p>​        K-means算法优点：</p><blockquote><p>（1）对待处理文本的输入顺序不太敏感</p></blockquote><blockquote><p>（2）对凸型聚类有较好结果</p></blockquote><blockquote><p>（3）可在任意范围内进行聚类。</p></blockquote><p>​        缺点：</p><blockquote><p>（1）对初始聚类中心的选取比较敏感，往往得不到全局最优解，得到的多是次优解</p></blockquote><blockquote><p>（2）关于算法需要预先设定的k值，限定了聚类结果中话题的个数，这在非给定语料的应用中并不可行</p></blockquote><blockquote><p>（3）该算法容易受到异常点的干扰而造成结果的严重偏差</p></blockquote><blockquote><p>（4）算法缺少可伸缩性</p></blockquote><p>  算法的描述如下：</p><p><img src="/images/img2.png" alt=""></p><p>  Kmeans的第二个缺点是致命的，因为在有些时候，我们不知道样本集将要聚成多少个类别，这种时候kmeans是不适合的，推荐使用hierarchical 或meanshift来聚类。第一个缺点可以通过多次聚类取最佳结果来解决。  </p><h3 id="2-层次法"><a href="#2-层次法" class="headerlink" title="2 层次法"></a>2 层次法</h3><p>​        这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案，即合并聚类（由下而上）和分裂聚类（由上而下）。</p><p>​        合并层次聚类是将语料库中的任一数据都当作一个新的簇，计算所有簇相互之间的相似度，然后将相似度最大的两个簇进行合并，重复这个步骤直到达到某个终止条件，因此合并聚类方法也被称为由下而上的方法。</p><p>​        分裂聚类恰好与合并聚类进行相反的操作，它是一种由上而下的方法，该方法先将数据集中所有的对象都归为同一簇，并将不断地对原来的簇进行划分从而得到更小的簇，直到满足最初设定的某个终止条件。</p><p>​        层次聚类法的优点:</p><blockquote><p>（1）适用于发现任意形状的簇</p></blockquote><blockquote><p>（2）适用于任意形式的相似度或距离表示形式</p></blockquote><blockquote><p>（3）聚类粒度的灵活性</p></blockquote><p>​    缺点：</p><blockquote><p>（1）算法终止的条件很模糊，难以精确表达并控制算法的停止</p></blockquote><blockquote><p>（2）一旦聚类结果形成，一般不再重新构建层次结构来提高聚类的性能</p></blockquote><blockquote><p>（3）难以处理大规模数据，也不能适应动态数据集的处理。</p></blockquote><p>​    由于层次聚类算法简单，因此针对它的研究也比较多，也提出了不少改进算法，主要方向就是将该策略与其他聚类策略相结合从而形成多层聚类。</p><p>​    代表算法有：BIRCH算法、CURE算法、CHAMELEON算法等。</p><h3 id="3-基于密度的方法"><a href="#3-基于密度的方法" class="headerlink" title="3:基于密度的方法"></a>3:基于密度的方法</h3><p>基于密度的方法与其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法智能发现“类圆形”的缺点。这个方法的指导思想就是，只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去。</p><p>​    代表算法有：DBSCAN算法、OPTICS算法、DENCLUE算法等。</p><p>​    典型的基于密度的算法是DBSCAN算法，该算法基本思想是：对于一个类中的每一个对象，在其给定半径R的区域中包含的对象数目不小于某一给定的最小数目，即在DBSCAN中，一个类被认为是密度大于一个给定阈值的一组对象的集合，能够被其中的任意一个核心对象所确定。DBSCAN算法执行时，先从数据集w中找到任意一个对象q，并查找w中关于R和最小下限数MinPts的从q密度到达的所有对象。如果q是核心对象，也就是说，q半径为R的领域中包含的对象数不少于MinPts，则根据算法可以找到一个关于参数R和MinPts的类。如果q是一个边界点，即q半径为R的领域包含的对象数小于MinPts，则没有对象从q密度到达，q被暂时标注为噪声点。然后，DBSCAN处理数据集W中的下一个对象。</p><p>DBSCAN中的几个重要概念：</p><ul><li>Ε邻域：给定对象半径为Ε内的区域称为该对象的Ε邻域；</li><li>核心对象：如果给定对象Ε邻域内的样本点数大于等于MinPts，则称该对象为核心对象；</li><li>直接密度可达：对于样本集合D，如果样本点q在p的Ε邻域内，并且p为核心对象，那么对象q从对象p直接密度可达。</li><li>密度可达：对于样本集合D，给定一串样本点p1,p2….pn，p= p1,q= pn,假如对象pi从pi-1直接密度可达，那么对象q从对象p密度可达。</li><li>密度相连：存在样本集合D中的一点o，如果对象o到对象p和对象q都是密度可达的，那么p和q密度相联。</li></ul><p>​        可以发现，密度可达是直接密度可达的传递闭包，并且这种关系是非对称的。密度相连是对称关系。DBSCAN目的是找到密度相连对象的最大集合。<br>Eg: 假设半径Ε=3，MinPts=3，点p的E邻域中有点{m,p,p1,p2,o}, 点m的E邻域中有点{m,q,p,m1,m2},点q的E邻域中有点{q,m},点o的E邻域中有点{o,p,s},点s的E邻域中有点{o,s,s1}.<br>那么核心对象有p,m,o,s(q不是核心对象，因为它对应的E邻域中点数量等于2，小于MinPts=3)；<br>点m从点p直接密度可达，因为m在p的E邻域内，并且p为核心对象；<br>点q从点p密度可达，因为点q从点m直接密度可达，并且点m从点p直接密度可达；<br>点q到点s密度相连，因为点q从点p密度可达，并且s从点p密度可达。</p><p>DBSCAN算法步骤如下：</p><p><img src="/images/img1.png" alt=""></p><p>   <strong>优点</strong>:</p><blockquote><p>与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。</p></blockquote><blockquote><p>与K-means方法相比，DBSCAN可以发现任意形状的簇类。</p></blockquote><blockquote><p>同时，DBSCAN能够识别出噪声点。</p></blockquote><blockquote><p>DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。</p></blockquote><p>​    DBSCAN聚类算法存在如下缺点：</p><blockquote><p>（1）随着对于大数据量的应用，需要有很大的内存支持与I/O开销。</p></blockquote><blockquote><p>（2）由于使用了全局参数R和MinPts，因此没有考虑数据密度和类别距离大小的不均匀性，所以DBSCAN算法很难得到高质量的聚类结果。</p></blockquote><blockquote><p>（3）算法参数过于复杂，领域外人员很难理解和掌握。</p></blockquote><h3 id="4-图论聚类法"><a href="#4-图论聚类法" class="headerlink" title="4  图论聚类法"></a>4  图论聚类法</h3><p>图论聚类方法最早是由Zahn提出的，又称作最大（小）支撑聚类算法。图论聚类要建立与问题相适应的图，图的节点对应于被分析数据的最小单元，图的边或者是弧对应于最小数据之间的相似性度量。因此，每个最小处理单元之间都会有一个度量的表达，这就确保数据局部特性比较易于处理。图论聚类法是以样本数据的局域链接特征作为聚类的主要信息源，因而其优点是易于处理局部数据的特性。</p><h4 id="图论聚类思想"><a href="#图论聚类思想" class="headerlink" title="图论聚类思想"></a><strong>图论聚类思想</strong></h4><p>  图论分析中，把待分类的对象想x1,x2,x3,x4……看做一个全连接无向图G=[X,E]中的节点，然后给每一条边赋值，计算任意两点之间的距离（例如欧氏距离）定义为边的权值。并生成最小支撑树，设置阈值将对象进行聚类分析。  </p><p><strong>算法步骤</strong></p><blockquote><p>利用prim算法构造最小支撑树。</p></blockquote><blockquote><p>给定一个阈值r,在最小支撑树中移除权值大于阈值的边，形成森林。</p></blockquote><blockquote><p>森林中包含剩下的所有的树。</p></blockquote><blockquote><p>每棵树视为一个聚类。</p></blockquote><h4 id="典型算法：AP算法"><a href="#典型算法：AP算法" class="headerlink" title="典型算法：AP算法"></a>典型算法：AP算法</h4><p>  <a href="https://www.cnblogs.com/huadongw/p/4202492.html" target="_blank" rel="noopener">AP聚类算法（Affinity propagation Clustering Algorithm ）</a>    是基于数据点间的”信息传递”的一种聚类算法。与k-均值算法或k中心点算法不同，AP算法不需要在运行算法之前确定聚类的个数。AP算法寻找的”examplars”即聚类中心点是数据集合中实际存在的点，作为每类的代表。  </p><h5 id="算法描述："><a href="#算法描述：" class="headerlink" title="算法描述："></a>算法描述：</h5><p>假设数据样${x_1,x_2,\cdots,x_n} $本集，数据间没有内在结构的假设。令$s$是一个刻画点之间相似度的矩阵，使得$s(i,j)&gt;s(i,k)$当且仅当$x_i$与$x_j$的相似性大于其与$x_k$的相似性。</p><p>AP算法进行交替两个消息传递的步骤，以更新两个矩阵：</p><blockquote><p>吸引信息（responsibility）矩阵$R：r(i,k)$ liuliu描述了数据对象<em>k</em>适合作为数据对象<em>i</em>的聚类中心的程度，表示的是从<em>i</em>到<em>k</em>的消息；</p></blockquote><blockquote><p>归属信息（availability）矩阵$A：a(i,k)$描述了数据对象i选择数据对象<em>k</em>作为其据聚类中心的适合程度，表示从<em>k</em>到<em>i</em>的消息。</p></blockquote><p>  两个矩阵<strong>R</strong> ,<strong>A</strong>中的全部初始化为0<strong>.</strong> 可以看成<em>Log</em>-概率表。这个算法通过以下步骤迭代进行：</p><p>  首先，吸引信息（responsibility)  ${r_{t + 1}}(i,k)$按照<br>$$<br>{r_{t + 1}}(i,k) = s(i,k) - \mathop {\max }\limits_{k’ \ne k} \{ {a_t}(i,k’) + s(i,k’)\}<br>$$<br>迭代。</p><p>  然后，归属信息（availability）  ${a_{t + 1}}(i,k)$ 按照<br>$$<br>{a_{t + 1}}(i,k) = \mathop {\min }\limits_{} \left( {0,{r_t}(k,k) + \sum\limits_{i’ \notin \{ i,k\} } {\max \{ 0,{r_t}(i’,k)\} } } \right),i \ne k<br>$$<br>和<br>$$<br>{a_{t+1}}(k,k) = \sum\limits_{i’ \ne k} {\max \{ 0,{r_t}(i’,k)\} }<br>$$<br>迭代。</p><p>  对以上步骤进行迭代，如果这些决策经过若干次迭代之后保持不变或者算法执行超过设定的迭代次数，又或者一个小区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。  </p><p>  为了避免振荡，AP算法更新信息时引入了衰减系数<em>λ</em>。每条信息被设置为它前次迭代更新值的<em>λ</em>倍加上本次信息更新值的1-<em>λ</em>倍。其中，衰减系数<em>λ</em>是介于0到1之间的实数。即第t+1次  $r(i,k)$,$a(i,k)$的迭代值<br>$$<br>{r_{t + 1}}(i,k) \leftarrow (1 - \lambda ){r_{t + 1}}(i,k) + \lambda {r_t}(i,k)<br>$$</p><p>$$<br>{a_{t + 1}}(i,k) \leftarrow (1 - \lambda ){a_{t + 1}}(i,k) + \lambda {a_t}(i,k)<br>$$</p><h3 id="5-基于网格的聚类"><a href="#5-基于网格的聚类" class="headerlink" title="5 基于网格的聚类"></a>5 基于网格的聚类</h3><h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><p>  基于网格的聚类算法首先将d维数据空间的每一维平均分割成等长的区间段,即把数据空间分割成一些网格单元。若一个网格单元中所含数据量大于给定的值,则将其定为高密度单元;否则将其视为低密度单元。如果一个低密度网格单元的相邻单元都是低密度的,则视这个低密度单元中的节点为孤立点或噪声节点。网格聚类就是这些相邻的高密度单元相连的较大集合。  </p><blockquote><p>将每个属性的可能值分割成许多相邻的区间,创建网格单元的集合(类似于对各维特征进行等宽或等深分箱).将每个样本映射到网格单元中,同时统计每个网格单元的信息,如样本数,均值,中位数,最大最小值,分布等.之后所有处理的基本单位都是网格单元.</p></blockquote><blockquote><p>删除密度低于某个指定阈值$\gamma$的网格单元.</p></blockquote><blockquote><p>由稠密的单元组形成簇</p></blockquote><h4 id="典型的网格聚类算法"><a href="#典型的网格聚类算法" class="headerlink" title="典型的网格聚类算法"></a>典型的网格聚类算法</h4><p>STING：基于网格多分辨率，将空间划分为方形单元，对应不同分辨率</p><p>CLIQUE：结合网格和密度聚类的思想，子空间聚类处理大规模高维度数据</p><p>WaveCluster：用小波分析使簇的边界变得更加清晰</p><h4 id="核心步骤"><a href="#核心步骤" class="headerlink" title="核心步骤"></a>核心步骤</h4><blockquote><p>1、 划分网格</p></blockquote><blockquote><p>2、 使用网格单元内数据的统计信息对数据进行压缩表达</p></blockquote><blockquote><p>3、 基于这些统计信息判断高密度网格单元</p></blockquote><blockquote><p>4、 最后将相连的高密度网格单元识别为簇</p></blockquote><h3 id="6-基于主题模型的聚类"><a href="#6-基于主题模型的聚类" class="headerlink" title="6 基于主题模型的聚类"></a>6 基于主题模型的聚类</h3><p>基于主题模型的聚类算法是假定数据的分布是符合一系列的概率分布，用概率分布模型去对数据进行聚类，而不是像层次聚类和划分聚类那样基于距离来进行聚类。因此，模型的好坏就直接决定了聚类效果的好坏。目前比较常用的基于主题聚类算法有LDA和PLSA等，其中LDA是PLSA的一个“升级”，它在PLSA的基础上加了Dirichlet先验分布，相比PLSA不容易产生过拟合现象，LDA是目前较为流行的用于聚类的主题模型。</p><h4 id="LDA（Latent-Dirichlet-Allocation，隐含狄利克雷分配）"><a href="#LDA（Latent-Dirichlet-Allocation，隐含狄利克雷分配）" class="headerlink" title="LDA（Latent Dirichlet Allocation，隐含狄利克雷分配）"></a>LDA（Latent Dirichlet Allocation，隐含狄利克雷分配）</h4><p>是一种三层贝叶斯概率模型，它由文档层、主题层和词层构成。LDA对三层结构作了如下的假设：</p><p>整个文档集合中存在k个相互独立的主题</p><blockquote><p>每一个主题是词上的多项分布</p></blockquote><blockquote><p>每一个文档由k个主题随机混合组成</p></blockquote><blockquote><p>每一个文档是k个主题上的多项分布</p></blockquote><blockquote><p>每一个文档的主题概率分布的先验分布是Dirichlet分布</p></blockquote><blockquote><p>每一个主题中词的概率分布的先验分布是Dirichlet分布 </p></blockquote><p> LDA可以被认为是如下的一个聚类过程： </p><blockquote><p>（1）各个主题（Topics）对应于各类的“质心”，每一篇文档被视为数据集中的一个样本。 </p></blockquote><blockquote><p>（2）主题和文档都被认为存在一个向量空间中，这个向量空间中的每个特征向量都是词频（词袋模型） </p></blockquote><blockquote><p>（3）与采用传统聚类方法中采用距离公式来衡量不同的是，LDA使用一个基于统计模型的方程，而这个统计模型揭示出这些文档都是怎么产生的。</p></blockquote><p>  它基于一个常识性假设：文档集合中的所有文本均共享一定数量的隐含主题。基于该假设，它将整个文档集特征化为隐含主题的集合，而每篇文本被表示为这些隐含主题的特定比例的混合。  </p><h5 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h5><blockquote><p>（1）隐含主题，形象的说就是一个桶，里面装了出现概率较高的单词，从聚类的角度来说，各个主题（Topics）对应于各类的“质心”，主题和文档都被认为存在于同一个词频向量空间中。</p></blockquote><blockquote><p>（2）在文档集合中的所有文本均共享一定数量的隐含主题的假设下，我们将寻找一个基于统计模型的方程。   </p></blockquote><p>  LDA的核心公式如下：<br>$$<br>p(w|d) = \sum_{i=1}^K{ p(w|z_k)*p(z_k|d) }<br>$$<br>$d$代表某篇文档，$w$代表某个单词，$z_k$代表第$i$主题，共有$K$个主题。通俗的理解是：文档$d$以一定概率属于主题$z_k$，即$p(z_k|d)$，而主题$z_k$下出现单词$w$的先验概率是$p(w|z_k)$，因此在主题$z_k$下，文档出现单词$w$的概率是$p(w|z_k)∗p(z_k|d)$，自然把文档在所有主题下$t_{i:K}$出现单词$w$的概率加起来，就是文档$d$中出现单词$w$的概率$p(w|d)$（词频）。 </p><p>  上面式子的左边，就是文档的词频，是很容易统计得到的。如果一篇文章中有很多个词语，那么就有很多个等式了。再如果我们收集了很多的文档，那么就有更多的等式了。这时候就是一个矩阵了，等式左边的矩阵是已知的，右边其实就是我们要求解的目标-与隐含主题相关，图示如下：   </p><p><img src="/images/img3.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>first blog</title>
      <link href="/2019/12/17/first-blog/"/>
      <url>/2019/12/17/first-blog/</url>
      
        <content type="html"><![CDATA[<p>欢迎来的我的第一个博客。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 第一个博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/12/17/hello-world/"/>
      <url>/2019/12/17/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
